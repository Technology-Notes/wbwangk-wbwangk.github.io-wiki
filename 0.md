Secure High-Rate Transaction Processing in

Bitcoin

(full version)

Yonatan Sompolinsky1 and Aviv Zohar1;2

1 School of Engineering and Computer Science,

The Hebrew University of Jerusalem, Israel

2 Microsoft Research, Herzliya, Israel

yoni sompo@cs.huji.ac.il, avivz@cs.huji.ac.il

Abstract. Bitcoin is a disruptive new crypto-currency based on a de-

centralized open-source protocol which has been gradually gaining mo-

mentum. Perhaps the most important question that will aﬀect Bitcoin’s

success, is whether or not it will be able to scale to support the high

volume of transactions required from a global currency system. We in-

vestigate the implications of having a higher transaction throughput on

Bitcoin’s security against double-spend attacks. We show that at high

throughput, substantially weaker attackers are able to reverse payments

they have made, even well after they were considered accepted by re-

cipients. We address this security concern through the GHOST rule,

a modi(cid:12)cation to the way Bitcoin nodes construct and re-organize the

block chain, Bitcoin’s core distributed data-structure. GHOST has been

adopted and a variant of it has been implemented as part of the Ethereum

project, a second generation distributed applications platform.

1 Introduction

Bitcoin is a disruptive protocol for distributed digital currency, which relies on

cryptographic elements to secure its operation. Since its initial launch in 2009

by its mysterious creator Satoshi Nakamoto, general interest in the currency has

been slowly increasing, and its uses have been slowly expanding.

While several obstacles such as regulatory uncertainty and an under-developed

infrastructure still need to be overcome, the main challenges that must be faced

from a computer science perspective are related to Bitcoin’s ability to scale to

higher transaction rates and to its ability to quickly process individual trans-

actions. This paper aims to address both of these issues and the connections

between them and Bitcoin’s security against double-spend attacks.

The core idea behind the Bitcoin protocol is to replace the centralized control

of money transmission ordinarily taken up by large organizations such as banks,

credit card companies, and other money transmitters, by a large peer-to-peer

network. The nodes of this network verify each other’s work and thus ensure

that no single entity is able to misbehave. Bitcoin achieves this by maintaining

a complete and public record of all its transactions at each node in the network.

This ledger, which is known as the block chain, is composed of a growing sequence

of blocks, each containing a set of approved transactions. The main challenge

that Bitcoin overcomes is the synchronization of the ledger between the various

nodes. Malicious parties may further try to interfere with this synchronization

in order to double-spend|to redirect previously processed payments that will

allow them to use the same money twice.

To help solve the double-spend problem blocks are required to contain a

proof-of-work, which is computationally diﬃcult to generate. The diﬃculty of

this task is adaptively set so that a block is created approximately once every

10 minutes in the entire network. Once created, blocks are propagated through

the network. The 10 minute interval allows blocks to (usually) propagate to

the vast majority of nodes before another block is created. If a node receives

two con(cid:13)icting blocks, which were created by distant nodes unaware of each

other’s work (or perhaps by a malicious attacker), it resolves the con(cid:13)ict by

picking the block pertaining to the longest block chain and adopting it. Satoshi

Nakamoto’s original analysis of the protocol [12] shows that as long as any

attacker holds less than 50% of the computational power in the network, the

probability that double-spend attacks succeed decreases exponentially with time,

which essentially allows payments to be considered accepted and irreversible

after some period. The analysis, however, assumes that blocks are sent across

the network much faster than they are created, and so it is ill-(cid:12)tted to a scenario

in which many transactions are processed by the network (which necessitates the

frequent creation of larger blocks, taking longer to transmit).

Indeed, capacity for additional transaction processing in Bitcoin is very much

needed. As of December 2014, Bitcoin’s network processes around 90 thousand

transactions per day [2], a number which has been slowly growing, but still

amounts to an average of roughly 1 transaction per second (TPS). In contrast,

Visa’s global payment system handled a reported 150 million transactions per

day in 2010 (just under 2000 TPS), and has grown steadily since. If Bitcoin is

not able to scale to appropriate rates that match demand, transaction fees will

rise, and users will be driven to use other forms of payment.

Bitcoin’s current low number of transactions is mainly due to its small user-

base. Once adoption increases, the system will need to scale to process trans-

actions at a higher rate, and previous security guarantees may no longer hold.

We investigate how susceptible the protocol is to double-spend attacks when

more transactions are processed per second. We note that larger block sizes or

more frequent block creation events (which are required in order to increase the

transaction throughput) result in more con(cid:13)icts between blocks, which severely

reduces the level of security from attacks.

To mitigate this, some methods for block compression were suggested by

members in the Bitcoin community, e.g., transmitting only transaction hashes in

blocks (an almost 16-fold reduction in size), or applying invertible Bloom lookup

tables to communicate the diﬀerences between the subsets of transactions nodes

are aware of [3]. Another approach is to use trustless oﬀ-chain transaction chan-

nels that slowly release money in minute portions to another party by updating

a transaction that is only committed to the block chain once a reasonable sum

of money has been transferred [1]. This approach has some downsides: money

must be locked and is unusable for the duration of the channel’s existence, it

only allows the aggregation of transactions between two parties that maintain

a channel, and (cid:12)nally, it is not always useful for other protocols built on top of

block chains (such as Ethereum) where individual updates cannot be aggregated

in a similar fashion.

We suggest an alternative to the longest-chain rule called GHOST, that

changes the con(cid:13)ict-resolution procedure for the block chain. GHOST selects

at each fork in the chain the heaviest subtree rooted at the fork. This proto-

col modi(cid:12)cation alleviates the above-mentioned security problem, and will help

block-chain-based protocols grow further. A variant of GHOST has been adopted

and implemented by the Ethereum project [4], a second generation distributed

applications platform that has recently received a great deal of attention. To best

utilize the capacity of the block chain all solutions should ideally be combined.

Our own improvement, GHOST, can be seen as a modi(cid:12)cation which allows an

increase in the protocol’s block chain commitments, which in turn, will allow

more transactions to take place at lower costs.

A second aspect of our work involves the time until the transaction is au-

thorized. As blocks are currently created on average once every 10 minutes, a

given transaction is only included in the chain after a relatively long amount

of time. Several alternative currencies that have forked the Bitcoin source-code

have modi(cid:12)ed this parameter and have set lower block creation rates (e.g., once

every 12 seconds in the case of FastCoin). We explore and quantify the secu-

rity implications of such choices, from lower resilience to attacks to the required

waiting time for a transaction to be considered accepted.

It is important to note that in addition to the decreased diﬃculty of a double-

spend attack, several other issues appear at high transaction rates: First, miners

that are better connected to the network enjoy rewards slightly larger than their

share of the hashing power, and second, the sel(cid:12)sh mining strategy explored by

Eyal and Sirer [8] can be employed by weaker miners. Both of these issues remain

unsolved by the GHOST protocol alone. In a companion paper [10] we explore

an additional modi(cid:12)cation (compatible with GHOST) that lowers the advantage

of highly-connected miners, and provides an additional increase in throughput.

2 Basics of the Bitcoin Protocol

The Block Chain. Bitcoin uses a public ledger to record the entire transaction

history, which essentially consists of a sequence of blocks, the block chain. New

blocks are created from time to time and are added successively to the ledger.

Each block contains the transactions that have occurred since the last block

and a cryptographic hash of the previous block in the sequence, which identi(cid:12)es

the predecessor uniquely.3 A transaction is considered con(cid:12)rmed only once it is

contained in some block which appears in this public log.4

The creation rate of blocks is set by requiring each block to contain a proof-of-

work in its header, in the form of a solution to a computationally diﬃcult problem

((cid:12)nding partial SHA-256 hash collisions). The problem depends on the most

recent block, and is solved by randomly trying diﬀerent inputs, thus ensuring

some (random) time lag between successful block creation events. The reader is

referred to [12] for a full explanation of the proof-of-work mechanism.

As the block chain, which represents the state of all \accounts", is kept locally

at each node, it is imperative that any update to the state of accounts will be

propagated to the entire network. Nodes which receive a transaction verify its

validity, and send it, in turn, to all their neighbors. Similarly, nodes which receive

a new block check its validity (i.e., its compatibility with all preceding blocks)

and transmit it to their neighbors.

The Formation and Resolution of Forks. Successive blocks are not nec-

essarily built atop one another, and thus they form a block tree rather than a

single chain (Fig. 3 illustrates such a scenario). One reason for the existence of

forks is the delay in the network: it is possible for two blocks to be created at

(about) the same time by far-away nodes in the network, in which case neither

will point at the other as its parent, and a fork occurs.

When faced with several (internally consistent) block chains each node in

the network is required to adopt only one as the valid account of transactions,

the \main chain". Bitcoin’s rule is simple: pick the longest chain (or in case of

ties, keep the one you received (cid:12)rst). An important property of the longest-chain

selection rule is that as time passes, all the nodes in the network will adopt the

same main chain. Indeed, in order for a fork in the block tree to last, two fractions

of the network need to successively create new blocks at about the same times,

a series of events which becomes rarer as time develops.

In addition to delays, forks can also occur due to a malicious deviation of a

node from the protocol. An attacker may choose to extend any arbitrary block,

and generate forks. The protocol cannot and does not deal with these forks

diﬀerently than with delay-induced ones; if the attacker manages to present a

longer chain of blocks, this chain will be accepted by other nodes in the network,

and the previous main chain will be abandoned.

Double-Spend Attacks. This method of overriding the main chain can be

used by an attacker to reverse transactions, a scheme called a \double-spend

attack". The attacker may pay some merchant and then secretly create a chain

of blocks without this payment that is longer than the network’s. By releasing

his chain he can trigger the replacement in the ledger which eﬀectively erases

3 Hash collisions are so rare that this hash can be regarded as a unique identi(cid:12)er of

the block.

4 Merely being included in a block is not suﬃcient to fully guarantee the irreversibility

of a transaction. Transactions become increasingly less likely to be reversed as more

blocks are added on top of them to the chain.

the transaction, or redirects the payment elsewhere (such an attack is illustrated

in Fig. 3).

The computational eﬀort required to create each block makes this attack a

diﬃcult undertaking, since the honest nodes usually have a great deal of com-

putational power, and the attacker must get very lucky if he is to replace long

chains.

However, if an attacker holds enough computational power he is able to gen-

erate blocks fast enough to bypass the main chain and override it, according to

the longest-chain selection rule. This enables him to reverse any transaction that

appears in the main chain at will. Speci(cid:12)cally, if the attacker has more compu-

tational power than the rest of the network combined, he is able to generate

blocks at a higher rate than the honest nodes and eventually to replace chains

of arbitrary length. This stronger form of attack is known in Bitcoin jargon as

\the 50% attack".5

3 The Model

We model the Bitcoin network as a directed graph G = (V; E). Each node v

∑

has some fraction pv (cid:21) 0 of the computational power of the entire network:

v2V pv = 1. Each individual node v in the network generates blocks according

to a Poisson process with a rate of pv (cid:1) (cid:21), so that the entire network combined

generates blocks at a Poisson process with rate (cid:21) (the protocol’s current value,

(cid:21) = 1

600 , was chosen by Satoshi at Bitcoin’s inception). We assume that each

edge e 2 E has a delay de associated with it, which is simply the time it takes

to send a block across it.

In the context of a network under attack, we will use (cid:21) = (cid:21)h as the honest

network’s block creation rate. The attacker’s rate is denoted relative to the

honest network by q (cid:1) (cid:21)h > 0, for some 0 < q < 1. In contrast to the honest

network, we assume that the attacker is creating long chains eﬃciently: its blocks

are always built on top of one another.6 See Appendix A for a more detailed

consideration of the relation between the attacker and the network.

For every block B, we denote by time(B) its (absolute) creation time. The

blocks essentially form a time-developing tree structure that is rooted at the

genesis block { the (cid:12)rst block created at the moment of Bitcoin’s inception; we

denote the structure of this tree at time t by tree(t), and by subtree(B) the

subtree rooted at B. Finally, the depth of block B in the tree will be denoted

depth(B).

5 The 50% attack owes its name to Satoshi’s result showing that the main chain is

secure (after suﬃcient waiting periods) as long as the attacker holds less than 50%

of the computational power. We show in this paper that in fact networks with delays

are more vulnerable and can be attacked with less computational power.

6 This essentially assumes that all computational assets held by the attacker are cen-

tralized and that blocks that it creates are transmitted instantly in its internal

network.

The structure of the block tree is aﬀected by the blocks that nodes point to

as their parent, and extend. Formally, we model this choice as a function s((cid:1))

which maps a block tree T = (VT ; ET ) to a block B 2 VT that is to be the parent

of the next block. Every node may posses a diﬀerent view of the tree (it may not

have heard of all created blocks) and thus applies s to its currently known tree.

The Bitcoin protocol currently requires nodes to build new blocks at the end

of the longest chain that is known to them. Accordingly, we denote by longest(t)

the deepest leaf in tree(t). Unless explicitly stated otherwise, we assume nodes

follow this rule.

The term \main chain" will correspond to the path from the genesis block

to the leaf that is selected for extension (usually longest(t)). The main chain is

considered by nodes to be the single accepted version of transaction history. Its

growth rate is therefore one of the core measures of the system’s performance.

Formally, the time it takes the main chain to advance from length n (cid:0) 1 to n is

n

a random variable that we denote as (cid:28)n. We denote (cid:28) = limn!1 1

i=1 (cid:28)n, and

n

(cid:12) = 1

E[(cid:28) ] . (cid:12) is the rate of block addition to the main chain, while (cid:21) is the rate

of block addition to the block tree.7

∑

Another parameter embedded in the protocol is the maximal block size (in

KB), denoted by b. We assume throughout the paper that there is high demand

for transaction processing and that blocks are always full to the limit.

Finally, we de(cid:12)ne the primary measure of Bitcoin’s scalability as the number

of transactions per second (TPS) the system adds to the history (the main

chain), in expectation. We denote by K the average number of transactions per

KB. The TPS is then: T P S((cid:21); b) := (cid:12)((cid:21); b) (cid:1) b (cid:1) K:

4 Reduced Security at High Throughput

In this section we explain why the Bitcoin protocol becomes more susceptible

to double-spend attacks when its throughput is increased. Assume an attacker

creates blocks at a rate of q (cid:1) (cid:21)h. If q (cid:1) (cid:21)h is greater than the growth rate of

the network’s main chain, (cid:12), the attack will always be successful (given enough

time), regardless of the current length of the chain it aims to bypass and replace

(by The Law of Large Numbers). Conversely, if q < (cid:12)

, the probability of the

(cid:21)h

attacker’s chain bypassing the main chain decreases exponentially as the main

chain grows in length (See Theorem 10 for the formal proof). We therefore think

of the ratio (cid:12)

(cid:21)h

as the \security threshold" of the system.

The throughput of the protocol is aﬀected by the two elementary parameters:

the block creation rate (cid:21), and the block size b. The diﬃculty of the computational

problem which is required to create a valid block can be lowered in order to

accelerate the block creation process. Similarly, larger blocks can be allowed to

propagate if one wishes to increase the block size. A na(cid:127)(cid:16)ve attempt at increasing

the throughput can be made by simply increasing both parameters. We argue

7 See Theorem 54, Chapter 2 in [17] for the compatibility of these two interpretations

of (cid:12).

that both of these modi(cid:12)cations lead to an increased number of forks in the block

tree, which in turn leads to a reduction of the security threshold of the system.

In other words, attackers can perform eﬀective attacks with less computational

power once the throughput is increased. The qualitative tradeoﬀs between these

parameters are depicted in Fig. 2.

Fig. 1. The relation between the block

size and the time it took to reach 25%

(red), 50% (green), and 75% (blue) of

monitored nodes, based on data pro-

vided by Decker and Wattenhofer [7].

Fig. 2. A general view of tradeoﬀs in

the Bitcoin protocol. Increasing the

block size or the block rate causes an

increase in the TPS, but also decreases

the security from double-spend attacks.

Larger Blocks. Indeed, while a node has not yet learned of the latest addition

to the main chain, any block that it creates will not add to that chain, but

rather contribute to a less updated alternative branch. Thus as the block size is

increased, blocks naturally take longer to propagate through the network, hence

more forks occur. This observation is well supported by a measurement study

conducted by Decker and Wattenhofer [7] who have measured block propagation

delays in the Bitcoin network. Figure 1, which is based on raw data that they

have generously shared with us, depicts a clear linear relation between the block

size and its propagation time.

Accelerated Block Creation. Similarly, if block creation is accelerated, more

blocks are being created by the honest network (larger (cid:21)h) while the most recent

block in the main chain is propagated. Again, these blocks will often be created

by nodes that are not fully up to date and will not extend the longest chain.

The attacker on the other hand, also creates blocks faster (at a rate of q (cid:1) (cid:21)h),

but does not suﬀer from a loss of eﬃciency.

Reduced Security. In both cases described above, blocks that are created do

not always contribute to the lengthening of the main chain, which makes it easier

for an attacker to replace it.

![R12](images/R12)

![R12](images/R12)

![R13](images/R13)

Figure 3 illustrates a scenario in which a highly forked block tree was created

by the honest network. The attacker secretly creates a chain of 6 blocks (denoted

1A, 2A,. . . , 6A) which is clearly longer than the network’s longest chain (ending

in block 5B). If block propagation was faster (in relation to the creation rate),

all blocks in the honest network’s tree would form a single long chain and would

not be overtaken by the attacker.

Fig. 3. A block tree in which the longest chain and the chain selected by GHOST

diﬀer. An attacker’s chain is able to switch the longest chain, but not the one selected

by GHOST.

5 The Greedy Heaviest-Observed Sub-Tree (GHOST)

In this section we present our main contribution to the protocol: a new policy for

the selection of the main chain in the block tree. The advantage of this suggested

change to the protocol is that it maintains the security threshold for successful

50% attacks at 1 (rather than (cid:12)

), even if the network suﬀers from extreme

(cid:21)h

delays and the attacker does not. This allows the protocol designer to set high

block creation rates and large block sizes without the fear of approaching the

50%-attack cliﬀ edge, which in turn implies that a high transaction throughput

can be securely maintained.

The basic observation behind the protocol modi(cid:12)cation that we suggest, is

that blocks that are oﬀ the main chain can still contribute to its weight. Consider,

for example, the block tree in Fig. 3. Block 1B is supported by blocks 2B, 2C,

and 2D that extend it directly, and include it in their chain. Similarly, blocks

3C, 3D, and 3E support both 1B and 2C as part of their chain. The heaviest

subtree protocol we suggest makes use of this fact, and adds additional weight

to blocks, helping to ensure that they will be part of the main chain.

Recall our de(cid:12)nition from Sect. 3; any node chooses the parent of its next

block according to a policy s(T ), that maps a tree T to a block in T which

essentially represents the main chain. Formally, our new protocol is a new parent-

selection policy. This new policy rede(cid:12)nes the main chain, which is what should

be regarded as the valid branch of transaction history.

For a block B in a block tree T , let subtree(B) be the subtree rooted at B,

and let ChildrenT (B) be the set of blocks directly referencing B as their parent.

Denote by GHOST (T ) the parent-selection policy we propose, de(cid:12)ned as the

output of the following algorithm.

Algorithm 1. Greedy Heaviest-Observed Sub-Tree (GHOST )

Input: Block tree T

1. set B   Genesis Block

2. if ChildrenT (B) = ∅ then return(B) and exit

3. else update B   argmax

jsubtreeT (C)j8

4. goto line 2

C2ChildrenT (B)

The algorithm follows a path from the root of the tree (the genesis block) and

chooses at each fork the block leading to the heaviest subtree. In the tree depicted

in Fig. 3, for instance, the subtree of block 1B contains 12 blocks, whereas that

of 1A contains only 6. The algorithm will thus pick 1B as belonging to the main

chain, and proceed to resolve the forks inside subtree(1B). This will result the

choice of blocks 0, 1B, 2C, 3D, 4B as the main chain of the tree (and not the

longest chain, ending in block 5B). This makes forks inside the subtree rooted at

1B of no consequence to the weight of block 1B itself | every addition of a block

to subtree(1B) makes it harder to omit it from the main chain. In particular,

when the attacker publishes its 6-blocks long secret chain, the same blocks as

before remain in the main chain.

5.1 Basic Properties of GHOST

It is imperative to (cid:12)rst show that all nodes eventually adopt the same history

when following GHOST. For every block B de(cid:12)ne by  B the earliest moment at

which it was either abandoned by all nodes, or adopted by them all. We call the

adoption of a block by all nodes the collapse of the fork.

Proposition 2 (The Convergence of History). P r( B < 1) = 1. In other

E[ B] < 1.

words, every block is eventually either fully abandoned or fully adopted. Moreover,

Proof. Let D be the delay diameter of the network. Assume that at time t >

time(B) block B is neither adopted by all nodes nor abandoned by all of them.

Denote by Et the event in which the next block creation in the system occurs

between times t + D and t + 2D, and then no other block is produced until time

t + 3D. We argue that once such an event occurs, block B is either adopted or

abandoned by all nodes. Indeed, between time t and t + D all nodes learn of all

8 We are in fact interested in the subtree with the hardest combined proof-of-work,

but for the sake of conciseness, we write the size of the subtree instead.

existing blocks (as no new ones are manufactured), and therefore each pair of

leaves (of the block tree) that have nodes actively trying to extend them must

have equal weight subtrees rooted at some common ancestor. A single block

is then created which breaks these ties, and another D time units allow it to

propagate to all nodes, which causes them to switch to a single shared history.

it doesn’t depend on t (as the exponential distribution is memoryless). Hence

Notice that P r(Et) is uniformly (in t) lower bounded by a positive number, as

the expected waiting time for the (cid:12)rst Et event is (cid:12)nite (see \Awaiting the

almost inevitable" in [19], Chapter 10.11). Finally, the stopping time  B is upper

bounded, by de(cid:12)nition, by the waiting time for the (cid:12)rst Et, implying E[ B] < 1.⊓⊔

We now show the main advantage of the GHOST chain selection rule, namely,

that it is resilient to 50% attacks, even at high rates or with signi(cid:12)cant delays

in the network: By waiting a suﬃciently long period of time (cid:28) after the block’s

creation, the probability that its status will change from \accepted" to \aban-

doned" can be made arbitrarily small.

Proposition 3 (Resilience to 50% Attacks). Assume the attacker’s block

creation rate is q (cid:1) (cid:21)h, and 0 (cid:20) q < 1. The probability that a block B will be oﬀ

the main chain sometime after time(B) + (cid:28) , given that it was in the main chain

at time(B) + (cid:28) , goes to zero as (cid:28) goes to in(cid:12)nity.

Contrast the statement above with the security threshold introduced in Sect. 4,

where q < (cid:12)

was required to guarantee resilience against 50% attacks. This

(cid:21)h

proposition suggests that in any network following the GHOST rule, the secu-

rity threshold is 1.

Proof (of Proposition 3). The event in which B is eventually discarded from

the main chain is contained in the event that a collapse has yet to occur (i.e.,

B (cid:21) time(B)+(cid:28) ). Relying again on the (cid:12)niteness of E[ B] (Proposition 2), and

applying Markov’s inequality, it follows that the probability that by time(B)+(cid:28) ,

B was either already abandoned or already adopted by all (honest) nodes goes

to 1, as (cid:28) goes to in(cid:12)nity. In the former case, the proposition holds trivially.

In the latter case, blocks are now built in B’s subtree at the rate of (cid:21)h, which

is higher than q(cid:21)h. Thus, as (cid:28) grows, the gap between the size of subtree(B)

and the attacker’s chain grows, making the probability of the attack succeeding

⊓⊔

sometime in the future arbitrarily low (The Law of Large Numbers).

The Rate of Collapse in GHOST. In Subsection 5.1 we have discussed

the collapse time  B for any block B and its implications to the growth and

convergence of the main chain in GHOST. Long living forks imply longer waiting

times until the entire network contributes con(cid:12)rmations to a block, and further

implies long waiting times for transaction authorization. It can prove useful to

further investigate how fast the collapse at B occurs. We do this for a simple

model including only two forks, each with equal contributing computational

power. Even this seemingly simple case proves to be non-trivial.

Theorem 4. Consider a network with two nodes, u and v, that equally create

blocks at a rate of (cid:21)=2, which are connected by a single link with delay d. For any

+

d(cid:21)

block B, E[nB] (cid:20) (d(cid:21))2

; where nB := jsubtreeT (B)j for T = tree( B).

The theorem gives an upper bound for the special con(cid:12)guration of two nodes;

we conjecture, however, that it is the worst case, and that in general setups

collapses occur even faster. Its proof appears in Appendix B.

6 Main Chain Growth in GHOST and in Longest-Chain

In this section we begin to systematically compare the two chain selection rules.

Central to this comparison is an analysis of the growth rate of the main chain ((cid:12))

under each one. Since this growth rate is highly dependent on the exact topology

of the network which is both unknown and extremely diﬃcult to measure, we

take a dual approach: First we bound the rates analytically from above and

below. Second, we simulate networks with randomly sampled overlay topologies

and measure the resulting block-trees. We then go on to discuss the implications

of these results in terms of security, throughput, and resource use of each rule.

6.1 A Lower Bound

We begin our analysis with the following approach: suppose that a cluster of

relatively well connected nodes (with delay diameter D) contains a fraction

0 (cid:20) (cid:11) (cid:20) 1 of the computational power of the entire network. In this case,

blocks created within this sub-network propagate internally relatively quickly,

and we can bound the rate of growth of the main chain from below. The bounds

are tight, both for longest-chain and for GHOST, and thus form a good basis

for comparison.

Lemma 5 (Longest-Chain & Bounded Delay). Let G=(V,E) be a network

graph (a sub-graph of the entire network) which generates blocks at a rate (cid:21)

=

(cid:11)(cid:1) (cid:21) with delay diameter D. Then under the longest-chain rule, the rate at which

the longest chain grows (cid:12)((cid:21)) (cid:21) (cid:21)

′

′

1+(cid:21)′(cid:1)D .

Lemma 6 (GHOST & Bounded Delay). Let G=(V,E) be a network graph

= (cid:11)(cid:1)(cid:21) with

(a sub-graph of the entire network) which generates blocks at a rate (cid:21)

delay diameter D. Then under the GHOST rule, the rate at which the longest

chain grows (cid:12)((cid:21)) (cid:21)

′

(cid:21)

′

1+2(cid:21)′(cid:1)D .

Both Lemma 5 and Lemma 6 can be shown to be tight. The bound is achieved

in a complete graph with n nodes, n ! 1, where the delay on all edges is exactly

D, and each node has 1=n’th of the computational power. This lower bound can

thus be thought of as approximating the ideal decentralized network, where the

computational power is well distributed among many equidistant nodes.

Lemma 5 follows, intuitively, from the fact that after some block U at depth n

(cid:21) seconds

was created and sent to all nodes (D seconds), it takes in expectation 1

′

′

for the next block U

was certainly aware

of the creation of U , its depth must be at least n + 1. The rate is thus lower

bounded by

1+(cid:21)′(cid:1)D . Refer to Appendix C for a formal proof.

to be created. As the creator of U

′

D+ 1

(cid:21)′ = (cid:21)

As GHOST does not select the longest chain, it can be expected that the

rate of growth of its main chain will be somewhat lower than in the longest-

chain rule. This is indeed the case. The loss in growth rate, however, is relatively

minor, and unlike in the longest-chain rule, has no bearing on the security of

GHOST. Lemma 6 follows as an immediate consequence of the following claim,

which is proven in Appendix D.

Claim 7. Let B be a block in tree T in a network as in Lemma 6, then regardless

of history, the expected waiting time for the creation of the last child of B is upper

bounded by 2D + 1

(cid:21)′ .

Application to Throughput (Under Longest-Chain) What recommenda-

tions should we give the designer of the system who wishes to set the protocol’s

parameters, given that the network’s topology is unknown? We now show how

some rather limited knowledge of the network’s topology could be used by the

designer to guarantee a certain measure of security.

Assume we have managed to measure the delay diameter of some fraction of

the network, namely, the maximal time D(b) it takes a block of size b to arrive

at some fraction (cid:11) of the network. Following the results depicted in Fig. 1, we

adopt a linear model of the delays; we thus assume that D(b) is of the form

D(b) = Dprop + Dbw (cid:1) b. Notice that Dprop is a measure of aggregate propagation

delay, and Dbw is an aggregate measure of bandwidth in units of seconds per

KB.

Lemma 8. Assume there exists a sub-network with a block creation rate of (cid:11)(cid:21)

and delay diameter D(b), in a network following the longest-chain rule. Then for

any x 2

, the protocol is able to achieve both a throughput of at least x

, through a right choice

(

)

0; K

Dbw

TPS and a security threshold of at least (cid:11) (cid:1)(

(

of the parameters b and (cid:21).

)

)

1 (cid:0) x(cid:1)Dbw

K

0; K

Dbw

Proof. By Lemma 5, the main chain grows at a rate of at least

the de(cid:12)nition of the throughput, T P S = b (cid:1) K (cid:1) (cid:12) (cid:21)

x 2

, there exists a large enough b = bx such that the RHS equals x

((cid:12)xing (cid:21)), thereby guaranteeing T P S (cid:21) x. The lower bound on (cid:12) then implies:

= 1 (cid:0) x (cid:1) Dbw

⊓⊔

1 + (cid:11)(cid:21) (Dprop + bx (cid:1) Dbw)

(cid:11)(cid:21) +D(b) . By

: For any

+ Dprop

bx(cid:1)Dbw

(cid:11)(cid:21)(cid:1)bx(cid:1)Dbw

= 1 (cid:0)

(cid:12)

(cid:11)(cid:21)

+Dbw

+Dprop

+ 1

(cid:21)

K

(cid:11)(cid:21)

b

K

:

Any evaluation of the real Bitcoin network’s behavior under higher through-

put requires full knowledge of the topology of the network. Unfortunately, the

structure is both unknown (partly because it is hard to measure, but also be-

cause miners attempt to keep their connections secret) and keeps shifting as

nodes connect and disconnect. To obtain an order of magnitude estimation we

apply Decker and Watenhoﬀer’s measurements of Bitcoin’s network to the bound

from Lemma 8.

The best linear (cid:12)t to the results, for (cid:11) = 0:5, yields a slope of Dbw = 0:066.

This implies, for instance, an achievable throughput of 15:15 TPS, coupled with

resilience to attackers with q up to 0:25 computational power.

Application of the Bound to GHOST (Eﬃciency) We have shown in

Proposition 3 that the security threshold in a network following GHOST is al-

ways 1. While this means there is no limiting security constraint (contrary to the

longest-chain case), the throughput cannot grow limitlessly: the transmission of

many blocks (only a fraction of which contribute to the main chain) consumes

bandwidth. Therefore, the ratio (cid:12)

(cid:21) is still of interest, not in a security context,

but rather as a measure of the network’s eﬃciency in its resource utilization.

put is at least (cid:11) (cid:1)(

Following the same method as previously, one can apply the linear delays

model to Lemma 6 and show that the network’s eﬃciency under a given through-

. E.g., the network is able to process 9.09

1 (cid:0) T P S(cid:1)2(cid:1)Dbw

)

K

transactions per second, while maintaining an eﬃciency of at least 0.2.

6.2 An Upper Bound

We proceed now to give upper bounds on the main chain’s growth rate. The

idea of the upper bound is to locate a partition of the network graph, such that

blocks take at least d time units to cross the partition (i.e., all links crossing

the cut have delay at least d). Given such a partition the network is inherently

ineﬃcient to some degree, as the communication delay between the two parts

may cause forks. The following theorem formalizes this:

Theorem 9. Let G=(V,E) be the network graph. Let S; T (cid:26) V be a partition of

the nodes such that 8s 2 S;8t 2 T we have dfs;tg (cid:21) d, and let pS; pT (pS ̸= pT )

be the fraction of computational power owned by nodes in S; T correspondingly.

Then both under longest-chain and under GHOST, the main chain’s growth rate

is bounded from above as follows: (cid:12)((cid:21)) (cid:20) (pS (cid:21))2epS (cid:21)2d(cid:0)(pT (cid:21))2epT (cid:21)2d

.

pS (cid:21)epS (cid:21)2d(cid:0)pT (cid:21)epT (cid:21)2d

The theorem is tight { networks consisting of only two nodes add blocks

to the main chain at exactly this rate. We defer the rather involved proof to

Appendix E.

6.3 Simulation Results

We simulated the growth of the main chain in networks roughly emulating the

topologies of Bitcoin’s P2P overlay network for nodes adhering either to longest-

chain or to GHOST. Following a behavior similar to the default in Bitcoin’s

reference client, each node initiates links to 8 uniformly selected neighbors (and

accepts all links others initiated). We simulate a network with 1000 nodes, and

assign computational power uniformly at random. The propagation delays on

the links were sampled from a normal distribution ((cid:22) = (cid:27) = 100 milliseconds).

Similarly, the bandwidth of each node was drawn from a normal distribution

((cid:22) = 1,(cid:27) = 0:2 MB). Both values were redrawn for negative results. The system

was later allowed to evolve as blocks were propagated by nodes. Figure 5 depicts

the security threshold measured in the system as a function of the block creation

rate. Figure 4 illustrates the resulting TPS in both cases, and shows that the

loss in eﬃciency of network resources caused by following the GHOST rule is

indeed relatively small. See further discussion in Subsection 6.1.

Fig. 4. T P S((cid:21))

Fig. 5. Security((cid:21))

7 Security Against Weak Attackers

We have so far considered only the eﬀect that delayed block propagation has

on the 50% attack. Even attackers with a modest block creation rate can still

succeed in a double-spend attack if they are lucky enough to generate many

blocks in a quick burst; Satoshi, in his original paper, analyzes this threat. His

analysis does not apply, however, to networks with non-negligible delay, and so

we revisit this question.

The Acceptance Policy in Longest-Chain. The process of transaction au-

thorization is de(cid:12)ned by an acceptance policy chosen by the recipient of funds.

Formally, the policy can be described as a function n(t; r; q), where r is the risk

the recipient is willing to tolerate, q the upper bound on the attacker’s fraction

of computational power, and t the time that elapsed since the transaction was

broadcast to the network. If the transaction receiver observes n blocks (\con(cid:12)r-

mations") atop his transaction by time t, he approves it only if n (cid:21) n(t; r; q), and

![140126519326480](images/140126519326480)

![140126518961616](images/140126518961616)

otherwise waits for n to increase.9 The policies for the GHOST and longest-chain

rules diﬀer. Notice however, that in both cases, if t seconds have passed since

the transaction was received, the probability that the attacker has completed k

blocks is (cid:16)k := e

. Thus, given some n; t we have a probability dis-

tribution on the initial gap between the attacker and the honest network. The

following theorem bounds the probability that an attacker will close this gap.

(cid:0)q(cid:21)ht (q(cid:21)ht)k

k!

Theorem 10. Consider a network G with delays. Let 1=(cid:12)1 be an upper-bound

on the expected waiting time for the next lengthening of the main chain, for all

possible states of the system. Let q(cid:21)h < (cid:12)1 be the creation rate of the attacker

(according to a Poisson process), and suppose the gap between the network’s

longest chain and that of the attacker is X0 blocks. Then the probability that the

attacker will succeed in extending its chain to be longer than the network’s is at

(

)X0+1

.

most

q(cid:21)h

(cid:12)1

The theorem is proved in Appendix F. This result justi(cid:12)es the following

acceptance policy:

{

n∑

(

)n(cid:0)k+1

1∑

}

n(t; r; q) := min

n

k=0

(cid:16)k (cid:1)

q(cid:21)h

(cid:12)1

(cid:16)k (cid:20) r

+

k=n+1

The (cid:12)rst term inside the parenthesis corresponds to the chance of the attacker

closing the gap (at some future time) given that at time t he is behind by n (cid:0) k

blocks. The second term aggregates the probability that its chain is long enough

at the moment of acceptance.

The Acceptance Policy in GHOST. In GHOST, a block B gains con(cid:12)rma-

tions from all blocks in its subtree. Once a collapse to a single subtree occurs,

further con(cid:12)rmations are added at a full rate of (cid:21)h. This justi(cid:12)es the following

policy:

(cid:20) r

B is the probability that at time t, block B has yet to be included

in the main chain of the entire honest network. The formulation given above

includes the event of a collapse. Subject to that occurrence, block B gains con-

(cid:12)rmations at a faster pace.

)n(cid:0)k+1

n(t; r; q) := minn

(∑

∑1

(1 (cid:0) (cid:17)t

B) (cid:1)

n

k=0 (cid:16)k (cid:1)

where (cid:17)t

+

k=n+1 (cid:16)k

+ (cid:17)t

B

{

)

(

q(cid:21)h

(cid:21)h

}

8 GHOST Implementation Details

Below we outline some additional details about the use and implementation of

the GHOST chain selection rule.

Links to Multiple Parents. As our protocol requires knowledge of oﬀ-chain

blocks by all nodes, we propose that their headers (but not necessarily their

9 Previous work, such as [12, 16], considered simpler policies that did not take elapsed

time into account.

entire contents) be propagated to all nodes. Information about oﬀ-chain blocks

can then be embedded inside each block by simply listing the hashes of other

childless blocks it is aware of.

Deployment. At low block creation rates, and with small block sizes, both

GHOST and the conventional longest-chain rule behave the same: all blocks

will simply be on a single long chain. Diﬀerences between the two rules appear

only at high throughputs. The adoption of GHOST can therefore be gradual at

low transaction rates{nodes will be partially compatible with the longest-chain

version as long as transaction rates do not increase (additional references to block

headers can be placed inside (cid:12)elds that the regular protocol currently ignores,

and so backward compatibility can be maintained). This point, however, is of

little importance. Increasing Bitcoin’s block size or the block creation rate will

require a hard fork in the protocol. Consequently, for these changes to take place

a majority of the mining power needs to accept them.

Retargeting (Diﬃculty Adjustment). Given potentially complex relations

between the growth rate of the main chain and the rate of created blocks, and the

fact that GHOST depends more on the total rate of block creation, we suggest

a change in the way automatic diﬃculty adjustments to the proof-of-work are

done. Instead of targeting a certain rate of growth for the longest chain, i.e.,

(cid:12) (which is Bitcoin’s current strategy), we suggest that the total rate of block

creation be kept constant ((cid:21)), which can be done, as the information on the

entire block tree is available following the links to all ancestor blocks. Notice

that the relation between (cid:12) and the diﬃculty is highly complex, and so Bitcoin’s

current targeting mechanism will malfunction at high rates.

Fees and Minted Coins. While GHOST does make use of oﬀ-chain blocks to

secure the protocol, we believe it is best to allocate minted coins only to the

creators of blocks that are on the main chain, similarly to how the longest chain

rule works today. The rate of minting can be adjusted independently from the

block creation rate (but in a very similar way) by adjusting the amount of minted

coins per block given the measured number of blocks in the recent past (e.g.,

in a 2 week window). A companion paper on Inclusive protocols [10] discusses

the inclusion of transactions from blocks that are oﬀ the main chain (and the

allocation of related fees).

Preventing Ampli(cid:12)ed Denial of Service Attacks. As each block in Bitcoin

is sent to the entire network by the nodes themselves, any burst of blocks may

disrupt the network. Attackers are naturally limited in their ability to create

recent blocks due to the proof-of-work requirement, but may try to create blocks

oﬀ-chain that are built upon blocks in the distant past (when the diﬃculty level

was low). This issue is handled by the current implementation using checkpoints

(points in the chain before which no additional oﬀ-chain blocks are accepted).

Other mechanisms that involve probabilistic proofs of combined diﬃculty (for

large chains that go back too far in the past) have also been suggested. Both

solutions can be adapted to GHOST as well.

9 Additional Related Work

The original security analysis done by Satoshi [12] has been improved in a

whitepaper published by Meni Rosenfeld [16]. Several papers have looked at

incentive concerns related to the operation of the protocol, examining issues re-

lated to transaction propagation [6], sel(cid:12)sh mining [8], and the distribution of

rewards within mining-pools [15]. Other works on Bitcoin have looked at its pri-

vacy aspects [13, 5], including analysis of its transaction graph [14] which allows

to de-anonymize some of its users. The Zerocoin protocol has been oﬀered as a

way to improve anonymity [11].

Our work deals, among other issues, with enabling fast con(cid:12)rmations for

transactions in the network. A paper by Karame et. al. discusses similar issues,

that relate to possible attacks on nodes that accept zero-con(cid:12)rmation transac-

tions [9]. They suggest several countermeasures that may help avoid such attacks.

Their work does not deal with an attack by an adversary with a signi(cid:12)cant block

creation rate, which can compute alternative chains on its own.

A paper closely related to ours is one that was published by Decker and

Wattenhofer, in which they present a measurement study of message propagation

times in the Bitcoin network. They associate delays with the creation of forks in

the block-tree, and with an increased vulnerability to the 50% attack [7]. As far

as we are aware, no other work addresses the issue of Bitcoin’s scalability, or its

security in a network with delayed block propagation.

10 Conclusion

This paper has focused primarily on the eﬀect network delays have on Bitcoin’s

security from double-spend attacks. In this context we presented GHOST, our

suggestion for the modi(cid:12)cation of the protocol, which helps secure Bitcoin when

processing transactions at high rates. Regarding the current state of the protocol,

we have given some theoretical security guarantees that can be applicable even if

limited information is known about the network topology. Our results underscore

the importance of the health of the network to Bitcoin’s security and scalability.

Many additional research questions should be addressed in light of our re-

sults: How should the block creation rate and block size dynamically adjust to

changing network conditions? Additionally, in Bitcoin so-called Simpli(cid:12)ed Proto-

col Veri(cid:12)cation nodes can operate without downloading the entire block chain. If

we are to increase the number of blocks per second, their job becomes harder. It

is therefore of great interest to create light nodes that can, for example, verify the

block chain probabilistically, without needing to download all headers. Finally,

it can be shown that in networks with delay that operate at high rates, large

miners get more than their fair share of the blocks, an eﬀect that skews rewards

in favor of large miners and slowly pushes the system towards a more centralized

one. One way to mitigate the problem, which can be applied to GHOST as well,

is presented in a companion paper on Inclusive protocols [10].

11 Acknowledgements

The authors were supported in part by the Israel Science Foundation (Grants

616/13, and 1773/13), and by the Israel Smart Grid (ISG) Consortium.

References

1. https://bitcoinj.github.io/working-with-micropayments

2. https://blockchain.info/charts/n-transactions

3. https://gist.github.com/gavinandresen/e20c3b5a1d4b97f79ac2

4. https://www.ethereum.org/

5. Androulaki, E., Karame, G.O., Roeschlin, M., Scherer, T., Capkun, S.: Evaluating

user privacy in bitcoin. In: Financial Cryptography and Data Security, pp. 34{51.

Springer (2013)

6. Babaioﬀ, M., Dobzinski, S., Oren, S., Zohar, A.: On bitcoin and red balloons. In:

The 13th ACM Conference on Electronic Commerce. pp. 56{73. ACM (2012)

7. Decker, C., Wattenhofer, R.: Information propagation in the bitcoin network. In:

13th IEEE International Conference on Peer-to-Peer Computing (P2P), Trento,

Italy (September 2013)

8. Eyal, I., Sirer, E.G.: Majority is not enough: Bitcoin mining is vulnerable. In:

Financial Cryptography and Data Security, pp. 436{454. Springer (2014)

9. Karame, G.O., Androulaki, E., Capkun, S.: Double-spending fast payments in bit-

coin. In: The 2012 ACM conference on Computer and communications security.

pp. 906{917. ACM (2012)

10. Lewenberg, Y., Sompolinsky, Y., Zohar, A.: Inclusive block chain protocols. In:

Financial Cryptography and Data Security. Springer (2015)

11. Miers, I., Garman, C., Green, M., Rubin, A.D.: Zerocoin: Anonymous distributed

e-cash from bitcoin. In: IEEE Symposium on Security and Privacy (2013)

12. Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system (2008)

13. Reid, F., Harrigan, M.: An analysis of anonymity in the bitcoin system. In: Security

and Privacy in Social Networks, pp. 197{223. Springer (2013)

14. Ron, D., Shamir, A.: Quantitative analysis of the full bitcoin transaction graph.

In: Financial Cryptography and Data Security, pp. 6{24. Springer (2013)

15. Rosenfeld, M.: Analysis of bitcoin pooled mining reward systems. arXiv preprint

arXiv:1112.4980 (2011)

16. Rosenfeld, M.: Analysis of hashrate-based double spending. arXiv preprint

arXiv:1402.2009 (2014)

17. Serfozo, R.: Basics of applied stochastic processes. Springer (2009)

18. Sompolinsky, Y., Zohar, A.: Bitcoin’s security model revisited. arXiv preprint

arXiv:1605.09193 (2016)

19. Williams, D.: Probability with martingales. Cambridge university press (1991)

A Where Is The Attacker in Longest Chain?

From a practical perspective, we must remember that a node listening to the

Bitcoin network does not really know the amount of computational power the

honest nodes in the network possess. In particular, the attacker may be building

blocks along with the network up until the time of the attack, or he may not.

Therefore, all that is observed is some amount of computational power which

triggers the reported block creation rate (cid:21)rep. We now ask ourselves what is the

worst case when using the longest-chain rule? An attacker who participates or

one that does not? Also, what is the right security threshold in terms of (cid:21)rep

(rather than (cid:21)h which is unknown)?

We begin with the assumption that the attacker has a fraction q of the

computational power of the honest network. Denote by (cid:21)a; (cid:21)h the block creation

rate of the attacker and the honest nodes respectively, and by (cid:21) = (cid:21)a + (cid:21)h

their joint rate. Our assumption is (cid:21)a < q(cid:21)h as before. (cid:21)rep is the observed

rate of block creation in the system (before the attack), which is in the range

[(cid:21)h; (cid:21)h + (cid:21)a]. The following proposition shows that for a given threshold q it is

enough to use (cid:21)rep as a measure of the honest network’s creation rate, as the

attacker would only make it harder on itself if it joined the rest of the network

and generated blocks before the attack. This is quite counter-intuitive, as the

attacker that adds to the rate before the attack fools the network into thinking

it is stronger. In reality, it increases the number of its blocks but lowers the

network’s eﬃciency, which is the true measure of resilience to attacks.

Proposition 11. If the network’s observed block rate is (cid:21)rep, for a given block

size, and (cid:12)((cid:21)rep) (cid:21) 2(cid:1)q

1+q (cid:21)rep, then the network is secure against an attacker with

computational power lower than q(cid:21)h. Furthermore, an attacker is most eﬀective

if it does not participate in block mining before the attack.

Proof. If a fraction f of the attacker’s blocks were included in (cid:21)rep prior to the

attack, then (cid:21)rep = (cid:21)h + f (cid:1) (cid:21)a: I.e., (cid:21)h = (cid:21)rep (cid:0) f (cid:1) (cid:21)a = (cid:21)rep (cid:0) f q(cid:21)h.

Observe that every block that the attacker publishes before the attack could

only increase the length of the main chain, and that the attacker’s maximal

contribution to the chain’s length is the number of blocks that he published.

Therefore: (cid:12)((cid:21)h) (cid:21) (cid:12)((cid:21)rep) (cid:0) f (cid:1) (cid:21)a. We obtain:

)

1 + q

1 + q

(cid:12) ((cid:21)h) (cid:21) (cid:12)((cid:21)rep) (cid:0) f (cid:1) (cid:21)a (cid:21) 2 (cid:1) q

2 (cid:1) q

(cid:1) ((cid:21)h + f (cid:1) (cid:21)a) (cid:0) f (cid:1) (cid:21)a =

1 + q

2 (cid:1) q

(cid:1) (cid:21)h (cid:0)

1 + q

2 (cid:1) q

(

(cid:1) (cid:21)h (cid:0)

1 + q

2 (cid:1) q

1 + q

1 (cid:0) 2 (cid:1) q

1 (cid:0) 2 (cid:1) q

)

1 + q

(cid:0) q

(

(

2 (cid:1) q2

1 + q

(cid:1) (cid:21)h =

(cid:1) (cid:21)h (cid:0)

(cid:1) (cid:21)rep (cid:0) f (cid:1) (cid:21)a =

2 (cid:1) q

1 + q

(

(cid:1) f (cid:1) q (cid:1) (cid:21)h (cid:21)

(cid:1) q (cid:1) (cid:21)h =

(

1 (cid:0) 2 (cid:1) q

(

q (cid:0) 2 (cid:1) q2

2 (cid:1) q + 2 (cid:1) q2 (cid:0) q (cid:0) q2

2 (cid:1) q

1 + q

)

)

1 + q

+

(cid:1) f (cid:1) (cid:21)a >

))

(cid:0)

1 + q

(cid:1) (cid:21)h =

(cid:1) (cid:21)h = q (cid:1) (cid:21)h > (cid:21)a:

1 + q

In conclusion, (cid:12)((cid:21)h) > (cid:21)a. The attacker’s chain thus grows slower than the

longest chain in the honest network’s tree.

The attacker is most eﬃcient if he avoids publishing his blocks before the

attack (f = 0), because these blocks can be used by him to increase the success-

⊓⊔

probability of double-spending. More on this in [18].

B Proof of Theorem 4

Theorem 4:

Consider a network with two nodes, u and v, and equal block creation rates (cid:21)=2,

which are connected by a single link with delay d. For any block B, E[nB] (cid:20)

(d(cid:21))2

; where nB := jsubtreeT (B)j for T = tree( B).

+

d(cid:21)

Proof. We de(cid:12)ne a state xn representing the time gap between the of creation

the n’th block by each of the nodes, in favor of u.

It is clear that whenever jxnj > d, a collapse has occurred, as this means a

message from u about a new block has arrived at v without the latter creating

a corresponding block in time, or vice versa.

In order to count nB, we recursively express the expected number of ad-

ditional blocks in subtree(B), given the current state xn. We denote this by

h(xn).

Given that the time gap xn+1 is positive, its value depends on the next

block creation of v, and thus follows an exponential distribution with rate (cid:21)=2;

the same argument applies to the case xn+1 < 0. If jxn+1j < d, the expected

addition to subtree(B) (conditioned on the current state) is simply 1 + h(xn+1),

otherwise, it is exactly 0. We express h() as a sum of two functions f (); g(). One

for the case in which the time gap increases in favor of u (f ), and one for the

case in which it decreases (g). By symmetry, the probability for these events is

2 . This justi(cid:12)es the following equations for f , g and h:

d∫

f (x) :=

g(x) :=

(cid:0)(cid:22)(t(cid:0)x)(h(t) + 1)dt = e(cid:22)x

(cid:22)e

(cid:0)(cid:22)(x(cid:0)u)(h(u) + 1)du = e

(cid:0)(cid:22)x

(cid:22)e

x

(cid:0)d

(cid:0)d

(cid:22)e

x∫

(cid:0)(cid:22)t h(t) + 1

dt

(cid:22)e(cid:22)u h(t) + 1

du

d∫

x∫

x

d∫

h(x) = f (x) + g(x):

Diﬀerentiating these functions we obtain,

(cid:0)(cid:22)t h(t) + 1

dt + e(cid:22)x (cid:1) (cid:0)1 (cid:1) (cid:22)e

(cid:0)(cid:22)x h(x) + 1

=

= (cid:22)f (x) (cid:0) (cid:22)

f (x) + g(x) + 1

=

(cid:22)e

= (cid:22)e(cid:22)x

df

dx

x

(cid:22)f (x) (cid:0) (cid:22)

h(x) + 1

(f (x) (cid:0) g(x) (cid:0) 1):

(cid:22)

Similarly,

(

)′

f

g

=

We thus arrive at the following linear non homogeneous diﬀerential system:

dg

dx

(cid:22)

(f (x) (cid:0) g(x) + 1)

)

(

)

=

(

(cid:0) (cid:22)

(cid:0) (cid:22)

(cid:22)

(cid:22)

(cid:1)

f

g

((cid:0) (cid:22)

)

+

;

(cid:22)

with the following boundary conditions:

Solving this system yields:

f (d) = 0; g((cid:0)d) = 0:

(

(

)

)

(d(cid:22))2 (cid:0) (x(cid:22))2 + 2d(cid:22) (cid:0) 2x(cid:22)

)

(d(cid:22))2 (cid:0) (x(cid:22))2 + 2d(cid:22) + 2x(cid:22)

(d(cid:22))2 (cid:0) (x(cid:22))2 + 2d(cid:22)

(

h(x) =

f (x) =

g(x) =

As the state at which the competition begins is x = 0, by symmetry, we

2 + d(cid:22)

⊓⊔

get that the expected number of blocks until a collapse is h(0) = (d(cid:22))2

blocks.

C Proof of Lemma 5

Lemma 5:

Let G=(V,E) be a network graph (a sub-graph of the entire network) which gen-

= (cid:11)(cid:1) (cid:21) with delay diameter D. Then under the longest-

′

erates blocks at a rate (cid:21)

chain rule, the rate at which the longest chain grows (cid:12)((cid:21)) (cid:21) (cid:21)

Proof. We follow a sequence of block creation events for blocks U0; U1; U2; : : :

such that each block Ui+1 is the (cid:12)rst block to be created after D seconds have

passed from the creation of the previous block Ui (so that there has been suf-

(cid:12)cient time to send Ui to all nodes in the network), i.e., the (cid:12)rst block B for

which time(B) (cid:0) D > time(Ui). Let us now make the following claim.

1+(cid:21)′(cid:1)D .

′

Claim 12. Let U0; U1; U2; : : : be a series of blocks that were created at least D

time units apart. Then for all n 2 N: Depth(Un) (cid:0) Depth(U0) (cid:21) n.

The claim can be proven by induction. It is trivially true for n = 0. Now we

assume that the claim is true for n = k, and show it is true for n = k + 1. by

time(Uk) we have Depth(Uk)(cid:0)Depth(U0) (cid:21) n. Then, consider the time at which

block Uk+1 is created. The node that created it has done so after hearing about

block Uk, it therefore has a chain that is at least of length k (by the induction

Now that we have established the claim, we can turn to calculating the

assumption and because Chains can only grow or be replaced by longer chains).

Therefore Uk+1 is built at depth that is at least 1 more than Uk.

lower-bound of (cid:12). Denote by Xi = time(Ui) (cid:0) time(Ui(cid:0)1) the random variable

representing the time between block creations. Notice that the Xi’s are i.i.d.

random variables (because the time interval they denote is exactly D time units

for the block to spread plus an exponentially distributed waiting time for the next

block’s creation somewhere in the network). Also note that (cid:12) (cid:21) E[ 1

(cid:0)1,

n

i=1 Xi. We therefore have

as the chain grows by at least n during the time

(cid:12) (cid:21) 1

E[X1] . Additionally, we know that E[X1] = D + E[Y ], where Y is a random

′

(cid:21)′ we

variable with an exponential distribution with parameter (cid:21)

have: (cid:12) (cid:21) 1

⊓⊔

. As E[Y ] = 1

n

i=1 Xi]

∑

∑

n

′

D+ 1

(cid:21)′ = (cid:21)

1+(cid:21)′(cid:1)D .

D Proof of Claim 7

Claim 7:

Let B be a block in tree T in a network as in Lemma 6, then regardless of history,

the expected waiting time for the creation of the last child of B is upper bounded

by 2D + 1

(cid:21)′ .

Proof. Let C be the (cid:12)rst block created after D seconds have passed from B’s

creation. Denote by (cid:28) the time from B’s creation until C has been created and yet

another D seconds elapsed. We argue that E[(cid:28) ] (cid:20) 2D + 1=(cid:21)

. This is easy to see:

It takes 1=(cid:21)

seconds in expectation to create block C, an event which can only

occur after D seconds have passed from B’s creation. Then, we deterministically

wait another D seconds to propagate C to the entire network.

′

′

We claim that after (cid:28) seconds from B’s creation, B will have no more children.

Let us examine the two possible cases:

Case I: C is a descendant of B. Once C has been propagated to all nodes, no

node considers B a leaf, and the GHOST chain selection rule only extends leaves

(in the subtree known to the extending node).

Case II: C is not a descendant of B. Because B was propagated to all nodes

before C was created, the node that extended C was well aware of B, but did

not extend it. It therefore had a strictly heavier subtree than B is part of after

the creation of C. D seconds later, block C is known to all other nodes, along

with its entire supporting subtree. In this case, B will not be extended directly

either { nodes have switched away from B if no other children extend it, or have

⊓⊔

switched to its descendants if it does have children.

E Proof of Theorem 9

Proof. In the above setting, v and u create blocks separately, and whenever one

completes a block it sends the message with its new block through the link,

to arrive at its counterpart d seconds later; in these d seconds the node still

continues with the attempt to build new blocks and lengthen its own version

of the main chain. Thus messages about blocks of the same depth (which were

created by u and v roughly at the same time) may simultaneously be traveling

in opposite directions on the link, causing a fork in the block-tree.

Note that the block-tree is actually a binary-tree | at any point of time their

are at most two branches not abandoned, as the two-node network contains at

most two con(cid:13)icting world views. Among two candidate chains, the heaviest one

and the longest one coincide; the analysis below applies thus equally to a network

following the \longest chain" rule and to that following GHOST.

In order to count the number of blocks that fail to enter the main chain, we

notice that such an event occurs precisely when two blocks of the same height

have been created.

Consider a block U of node u. We say that the window of U is created d

time units before U ’s creation, and is gone d time units after it. Notice, that

a block U is built upon any of v’s blocks that was created before U ’s window

was created, and also that block U arrives at node v exactly at the end of U ’s

window.

We say that U is \threatened" at a given time, if U ’s window has been

created, and the chain at v is of length depth(U ) (cid:0) 1 (this time interval is

contained in U ’s window). During this period, the next block created by v will

be of the same depth as U and one of the blocks is wasted. We de(cid:12)ne open as

the time that elapsed from U ’s window’s creation to the moment at which it

becomes threatened, and de(cid:12)ne close as the time that elapsed from its window’s

creation until it ceases to be threatened.

Notice that the closure of U can occur in two ways: either 2d time has passed

from the U -window creation, and v received a message containing U , or v gener-

ated a competing block of the same height before that. Therefore, the diﬀerence

between the moment U is opened and the moment it is closed is between 0 and

2d. In addition, notice that two blocks of u cannot be simultaneously threatened

(v’s chain cannot be shorter by 1 from both their depths at the same time).

of a continuous part on the region (0; 2d], and a discrete part on the atomic

Assuming block Un’s window was created at a time that we shall denote as

time 0, open(Un) and close(Un) are random variables taking values in [0; 2d], for

whom we have close(Un) (cid:21) open(Un). The distribution of open(Un) is composed

event fopen(Un) = 0g. We denote the former by (cid:11)n(x), for x 2 (0; 2d], and the

latter by (cid:11)n;0. Similarly, close(Un)’s probability distribution has a continuous

part which we denote !n(x) on [0; 2d), and a discrete part !n;2d for the atomic

event fclose(Un) = 2dg.10

We denote by fS and fT the pdf’s of the exponential random variables with

rates pS(cid:21) and pT (cid:21), respectively. We claim that the following relations hold:

10 We avoided de(cid:12)ning the pdf’s (cid:11)n and !n on the entire closed segment [0; 2d], al-

though it can be done by continuity; if de(cid:12)ned so, one needs to be careful to distin-

guish between (cid:11)n(0) and (cid:11)n;0 (respectively between !n;2d, and !n(2d)) which are

diﬀerent in essence.

(cid:11)n(x) =

x

2d

∫

!n(cid:0)1(y) (cid:1) fS(y (cid:0) x)dy +

!n(cid:0)1;2d (cid:1) fS(2d (cid:0) x);

∫

(cid:11)n;0 (cid:1) fT (x);

(cid:11)n(z) (cid:1) fT (x (cid:0) z)dy +

0 (cid:20) x < 2d

x

0 < x (cid:20) 2d

!n(x) =

(1)

(2)

Indeed, starting with 1, Un opens x seconds after the window creation if and

only if for some y, Un(cid:0)1 closed y seconds after its window creation (with prob-

ability !n(cid:0)1(y) for y < 2d and !n(cid:0)1;2d for y = 2d), and the gap between their

respective creations was y (cid:0) x seconds (fS(y (cid:0) x)). This calculation is relevant

only to x > 0, as only under the assumption that Un’s window creation preceded

Un(cid:0)1’s closure the period between Un(cid:0)1’s opening and closing (y) contains that

between Un’s window creation and opening (x).

Regarding 2, Un closes x seconds after its window creation if and only if for

some z, z seconds passed between Un’s window creation and its opening (with

probability (cid:11)n(z) for z > 0 and (cid:11)n;0 for z = 0), and x (cid:0) z seconds between the

latter and its closing (fT (x (cid:0) z)). That the gap between the opening and the

closing of Un is controlled by fT is true only in the region x < 2d.

The processes open(Un) and close(Un) are Markovian, and we now write

equations 1 and 2 applied to their limiting distributions, (cid:11)(x); (cid:11)0 and !(x); !2d:

These equations resolve to a diﬀerential equation system:

whose solution is:(

)

0 < x (cid:20) 2d

x

2d

x

(cid:11)(x) =

!(x) =

!(y) (cid:1) fS(y (cid:0) x)dy +

∫

!2d (cid:1) fS(2d (cid:0) x);

∫

(cid:11)0 (cid:1) fT (x);

(

pS(cid:21) (cid:0) pS(cid:21)

pT (cid:21) (cid:0) pT (cid:21)

(

(cid:11)(x)

!(x)

for A = (cid:11)(0) (cid:0) !(0)

(cid:11)(z) (cid:1) fT (x (cid:0) z)dy +

0 (cid:20) x < 2d

(

)

)

(

)

pS(cid:21)(eS(cid:1)x (cid:0) 1)

pT (cid:21)(eS(cid:1)x (cid:0) 1)

)′

(cid:11)

!

)

(

=

A

S

(cid:1)

(cid:11)

!

=

+

(cid:11)(0)

!(0)

; S = pS(cid:21) (cid:0) pT (cid:21):

(3)

(4)

(5)

Lemma 13. Equation 5 implies

pS(cid:21) (cid:0) pT (cid:21)

!2d =

pS(cid:21) (cid:0) pT (cid:21)e(cid:0)(pS (cid:21)(cid:0)pT (cid:21))2d

:

By the de(cid:12)nition of !2d, it is precisely the fraction of u’s blocks that have

no con(cid:13)icting blocks created by v. The blocks which contribute to the growth of

the main chain are can thus be counted by considering all of v’s blocks as valid,

and adding to those all of u’s non-con(cid:13)icting blocks. Altogether, we obtain

(cid:12)((cid:21)) = pT (cid:21) + !2d (cid:1) pS(cid:21) =

pS(cid:21) (cid:0) pT (cid:21)

pT (cid:21) +

pS(cid:21) (cid:0) pT (cid:21)e(cid:0)(pS (cid:21)(cid:0)pT (cid:21))2d

(pS(cid:21))2epS (cid:21)2d (cid:0) (pT (cid:21))2epT (cid:21)2d

pS(cid:21)epS (cid:21)2d (cid:0) pT (cid:21)epT (cid:21)2d

pS(cid:21) =

:

⊓⊔

This concludes the proof of Theorem 9.

All that remains is to prove Lemma 13

Proof (of Lemma 13). By equation 1, (cid:11)(2d) = !2dpS(cid:21), and by 2, !(0) = (cid:11)0pT (cid:21).

Therefore,

!

!(x) = ^ApT (cid:21)(e(pS (cid:21)(cid:0)pT (cid:21))x (cid:0) 1) + (cid:11)0pT (cid:21), for ^A :=

(x) = pT (cid:21)Ae(pS (cid:21)(cid:0)pT (cid:21))x:

′

By E, !

(cid:11)(x) (cid:0) !(x) = Ae(pS (cid:21)(cid:0)pT (cid:21))x ) (cid:11)

(x) = pT (cid:21)((cid:11)(x) (cid:0) !(x)); and therefore,

x∫

x∫

′

′

(x) = pS(cid:21)((cid:11)(x) (cid:0) !(x)) )

A

pS(cid:21) (cid:0) pT (cid:21)

)

(cid:11)(x) =

pS(cid:21)((cid:11)(t) (cid:0) !(t))dt + (cid:11)(0) = pS(cid:21)

Ae(pS (cid:21)(cid:0)pT (cid:21))tdt + (cid:11)(0) =

(e(pS (cid:21)(cid:0)pT (cid:21))x (cid:0) 1) + (cid:11)(0):

pS(cid:21) (cid:1) A

pS(cid:21) (cid:0) pT (cid:21)

We obtain,

(cid:11)(0) = !(0) + A = (cid:11)0pT (cid:21) + A =)

(cid:11)(x) =

(cid:11)(2d) =

pS(cid:21) (cid:1) A

(e(pS (cid:21)(cid:0)pT (cid:21))x (cid:0) 1) + (cid:11)0pT (cid:21) + A =)

pS(cid:21) (cid:0) pT (cid:21)

pS(cid:21) (cid:1) A

(E (cid:0) 1) + (cid:11)0pT (cid:21) + A;

pS(cid:21) (cid:0) pT (cid:21)

pS (cid:21)(cid:0)pT (cid:21) and E := e(pS (cid:21)(cid:0)pT (cid:21))2d,

(cid:11)(2d) (cid:0) (cid:11)0pT (cid:21)

pS(cid:21)E (cid:0) pT (cid:21)

!2dpS(cid:21) (cid:0) (cid:11)0pT (cid:21)

pS(cid:21)E (cid:0) pT (cid:21)

=

A

:

^A =

Therefore, for ^A :=

We have thus obtained explicit expressions for (cid:11)(x) and !(x) subject to the

parameters (cid:11)0 and !2d:

(cid:11)(x) = ^A(pS(cid:21)e(pS (cid:21)(cid:0)pT (cid:21))x (cid:0) pT (cid:21)) + (cid:11)0pT (cid:21)

!(x) = ^A(pT (cid:21)e(pS (cid:21)(cid:0)pT (cid:21))x (cid:0) pT (cid:21)) + (cid:11)0pT (cid:21)

By the de(cid:12)nition of (cid:11) we know that (cid:11)’s integral over the range (0; 2d] should

be 1 (cid:0) (cid:11)0:

2d∫

(

)

^A(pS(cid:21)e(pS (cid:21)(cid:0)pT (cid:21))t (cid:0) pT (cid:21)) + (cid:11)0pT (cid:21)

dt =

1 (cid:0) (cid:11)0 =

(

^A

)

+ 2d (cid:1) (cid:11)0pT (cid:21) =

(cid:0) 2d (cid:1) pT (cid:21)

)

pS(cid:21)(E (cid:0) 1)

(

pS(cid:21) (cid:0) pT (cid:21)

^E (cid:0) 2d (cid:1) pT (cid:21)

(

^A

+ 2d (cid:1) (cid:11)0pT (cid:21);

)

^E (cid:0) 2d (cid:1) pT (cid:21)

pS (cid:21)(cid:0)pT (cid:21) . Therefore,

(cid:11)0 = 1 (cid:0) !2dpS(cid:21) (cid:0) (cid:11)0pT (cid:21)

pS(cid:21)E (cid:0) pT (cid:21)

for ^E := pS (cid:21)(E(cid:0)1)

(cid:0) 2d (cid:1) (cid:11)0pT (cid:21):

Similarly, the integral of ! over [0; 2d) should be 1(cid:0) !2d, and combining this

with the relation (cid:11)(x) (cid:0) !(x) = Ae(pS (cid:21)(cid:0)pT (cid:21))x we obtain:

2d∫

Ae(pS (cid:21)(cid:0)pT (cid:21))tdt = ^A(E (cid:0) 1) =)

!2dpS(cid:21) (cid:0) (cid:11)0pT (cid:21)

pS(cid:21)E (cid:0) pT (cid:21)

(E (cid:0) 1) =)

= 1 (cid:0)

(E (cid:0) 1) =)

(E (cid:0) 1) =)

pT (cid:21)

pS(cid:21)E (cid:0) pT (cid:21)

1 (cid:0) (cid:11)0 (cid:0) (1 (cid:0) !2d) =

!2d

(cid:11)0

!2d

(cid:11)0

!2d (cid:0) (cid:11)0 = ^A(E (cid:0) 1) =

pS(cid:21) (cid:0) pT (cid:21)

(

)

pS(cid:21)E (cid:0) pT (cid:21)

!2d

(cid:11)0

(cid:0) 1 =

1 (cid:0) pS(cid:21)(E (cid:0) 1)

pS(cid:21)E (cid:0) pT (cid:21)

1 (cid:0)

!2d

(cid:11)0

=

pT (cid:21)

pS (cid:21)E(cid:0)pT (cid:21) (E (cid:0) 1)

1 (cid:0) pS (cid:21)(E(cid:0)1)

pS (cid:21)E(cid:0)pT (cid:21)

= E

We conclude with,

(

(

(

!2d = E (cid:1) (cid:11)0 = E

E

1 (cid:0) !2d

E

E pT (cid:21)

1 (cid:0) !2dpS(cid:21) (cid:0) !2d

)

pS(cid:21)E (cid:0) pT (cid:21)

(cid:0) 2d (cid:1) !2d

E

pT (cid:21)

^E (cid:0) 2d (cid:1) pT (cid:21)

!2d =

E

1 + ^E

=

e(pS (cid:21)(cid:0)pT (cid:21))2d

pS (cid:21)(cid:0)pT (cid:21)

1 + pS (cid:21)(e(pS (cid:21)(cid:0)pT (cid:21))2d(cid:0)1)

pS(cid:21) (cid:0) pT (cid:21)

=

pS(cid:21) (cid:0) pT (cid:21)e(cid:0)(pS (cid:21)(cid:0)pT (cid:21))2d

:

)

pT (cid:21)

=

)

(

)

^E (cid:0) 2d (cid:1) pT (cid:21)

= E (cid:0) !2d ^E =)

e(pS (cid:21)(cid:0)pT (cid:21))2d

(cid:0) 2d (cid:1) !2d

E

=

pS(cid:21)e(pS (cid:21)(cid:0)pT (cid:21))2d (cid:0) pT (cid:21)

(cid:1) (pS(cid:21) (cid:0) pT (cid:21))

⊓⊔

F Proof of Theorem 10

Theorem 10:

Consider a network G with delays. Let 1=(cid:12)1 be an upper-bound on the expected

waiting time for the next lengthening of the main chain, for all possible states

of the system. Let q(cid:21)h < (cid:12)1 be the creation rate of the attacker (according to

a Poisson process), and suppose the gap between the network’s longest chain

and that of the attacker is X0 blocks. Then the probability that the attacker

will succeed in extending its chain to be longer than the network’s is at most

(

q(cid:21)h

(cid:12)1

)X0+1

.

The proof depends on the following two lemmas:

Lemma 14. Let & be a nonnegative random variable with increasing hazard rate

function. Then, 8k 2 N

E[& k] (cid:20) k!Ek[&]:

Note that the almost inverse inequality, E[& k] (cid:21) Ek[&], stems from Jensen’s

inequality.

(cid:0)1.

Lemma 15. Let & be as in Lemma 14, let f be its pdf, and denote (cid:12) := E[&]

Then for any constant 0 < (cid:13) < (cid:12) the function H(cid:13);(cid:12) obtains a positive root a0

smaller than (cid:13)

(cid:12) ; where

H(cid:13);(cid:12)(a) :=

f (&)e(cid:13)( 1

a

(cid:0)1)& d& (cid:0) 1

a

:

∫ 1

∫ 1

∫ 1

Proof (of Lemma 14). By induction on k. The base case k = 0 is trivial. For

k + 1 we have:

& k+1f (&) =

∫ 1

E[& k+1] =

[& k+1 (cid:1) (cid:0)e

(cid:0)(cid:3)(&)]

0 +

& k+1(cid:21)(&)e

(cid:0)(cid:3)(&)d& =

(k + 1)& ke

(cid:0)(cid:3)(&)d& = (k + 1)

∫ 1

(cid:0)(cid:3)(&)d&

(cid:21)(&)e

& k

(cid:21)(&)

On the other hand,

∫ 1

∫ 1

&f (&) =

E[&] =

(cid:1) (cid:0) e

[&

(cid:0)(cid:3)(&)]

0 +

&(cid:21)(&)e

(cid:0)(cid:3)(&)d& =

e

(cid:0)(cid:3)(&)d& =

∫ 1

(cid:0)(cid:3)(&)d&;

e

∫ 1

∫ 1

and therefore,

(k + 1)!Ek+1[&] = (k + 1)k!Ek[&]E[&] =

(cid:0)(cid:3)(&)d& = (k + 1)

(k + 1)k!Ek[&]

e

It is thus suﬃcient to prove that,

∫ 1

(cid:0)(cid:3)(&)d& (cid:20) (k + 1)

(cid:21)(&)e

& k

(cid:21)(&)

∫ 1

∫ 1

k!Ek[&]

(cid:21)(&)

(cid:0)(cid:3)(&)d&

(cid:21)(&)e

k!Ek[&]

(cid:21)(&)

(cid:0)(cid:3)(&)d&;

(cid:21)(&)e

& k (cid:0) k!Ek[&]

(cid:21)(&)

(cid:0)(cid:3)(&)d& (cid:20) 0:

(cid:21)(&)e

Using the induction hypothesis we obtain:

(k + 1)

or, equivalently, that ∫ 1

∫ 1

∫

& k (cid:0) k!Ek[&]

(cid:21)(&)e

& k (cid:0) k!Ek[&]

(cid:21)(&)

k

(k!Ek[&])

(cid:20)

(cid:21)(&)

∫

∫ 1

∫ 1

(cid:21)((k!Ek[&]) 1

k )

(cid:21)((k!Ek[&]) 1

k )

(cid:21)((k!Ek[&]) 1

k )

(cid:0)(cid:3)(&)d& =

(cid:0)(cid:3)(&)d& +

(cid:21)(&)e

∫ 1

& k (cid:0) k!Ek[&]

(k!Ek[&])

k

(cid:21)(&)

(cid:0)(cid:3)(&)d&

(cid:21)(&)e

(k!Ek[&])

k

(& k (cid:0) k!Ek[&])(cid:21)(&)e

(cid:0)(cid:3)(&)d& +

(& k (cid:0) k!Ek[&])(cid:21)(&)e

(cid:0)(cid:3)(&)d& =

k

(k!Ek[&])

(& k (cid:0) k!Ek[&])(cid:21)(&)e

(cid:0)(cid:3)(&)d& (cid:20) 0;

where we used (cid:21)’s monotonicity in the (cid:12)rst inequality and the induction hypoth-

⊓⊔

esis in the last one.

Proof (of Lemma 15). We have H(cid:13);(cid:12)(0) = 1. H(cid:13);(cid:12) is continuous in a and thus,

by The Intermediate Value Theorem, it suﬃces to show that H(cid:13);(cid:12)( (cid:13)

(cid:12) ). We need to show that H((cid:13)) (cid:20) 0, and we do so by showing

Let H((cid:13)) := H(cid:13);(cid:12)( (cid:13)

that its Taylor series elements (around (cid:12)) are all (!) nonpositive. That is, we show

(cid:20) 0; and this would imply, H((cid:13)) = (cid:6)

(cid:20) 0.

that H (k)((cid:12)) ((cid:13)(cid:0)(cid:12))k

((cid:13) (cid:0) (cid:12))k

(cid:12) ) (cid:20) 0.

k=0H (k)((cid:12))

k!

k!

Indeed,

{∫ 1

}

=

(cid:13)=(cid:12)

(cid:0)1)t

(cid:13)

(cid:12)

dt (cid:0) 1

}

=

(cid:13)( 1

(cid:13)

(cid:12)

}

f (t)e

dk

d(cid:13)k

f (t)e((cid:12)(cid:0)(cid:13))tdt (cid:0) (cid:12)

(cid:13)

(cid:0)1)tdt +

a

((cid:0)t)kf (t)e(cid:13)( 1

dk

d(cid:13)k

H (k)((cid:12)) =

{∫ 1

{∫ 1

∫ 1

((cid:0)t)kf (t)dt + k!(cid:12)

(cid:0)k((cid:0)1)k+1:

(cid:13)=(cid:12)

k!(cid:12)

((cid:0)(cid:13))k+1

=

(cid:13)=(cid:12)

As the Taylor elements of H((cid:13)) are of alternating signs (recall (cid:13) < (cid:12)), it

suﬃces to show the inequalities H (k)((cid:12)) (cid:20) 0 and H (k)((cid:12)) (cid:21) 0 for even and odd

(cid:0)k;

k’s respectively. It is suﬃcient to show that for all k:

⊓⊔

tkf (t)dt (cid:20) k!(cid:12)

which was proven in Lemma 14.

∫ 1

(cid:0)k

(we will make use of this inequality later).

Proof (of Theorem 10). Let (cid:28)n be the waiting time for the nth lengthening of

the main chain. Let f(cid:28)nj(cid:28)n(cid:0)1;:::;(cid:28)1 be the conditional pdf of (cid:28)n given (cid:28)n(cid:0)1; :::; (cid:28)1.

Denote (cid:12) := E[(cid:28)n j (cid:28)n(cid:0)1; :::; (cid:28)1]

(cid:0)1 for some given history (that is, for some

realization of the (cid:28)i’s up to n (cid:0) 1). By our assumption, (cid:12) (cid:21) (cid:12)1, and thus 8k 2

N; (cid:12)

(cid:0)k (cid:20) (cid:12)

The random variable (cid:28)n given a history is nonnegative with increasing hazard

rate. Indeed, when a node creates a new block it broadcasts it to the network, and

as more and more nodes learn about it, more computational power is contributed

to the eﬀort of creating the next one and thereby lengthening the main chain.

If meanwhile a con(cid:13)icting block was created elsewhere, still more computational

power is working on lengthening the main chain, just on a diﬀerent version of it.

of this process, namely, N2(t) := maxfn j ∑

∑

The attacker’s chain is built according to a Poisson process in the worst

case, whose rate we denoted by (cid:13). Let N2 be the event-count (random variable)

j=1 (cid:28)n (cid:20) tg. De(cid:12)ne, Xn := n (cid:0)

)Xn

(

n

N2(

n

j=1 (cid:28)n), and Yn :=

(cid:13)

(cid:12)1

:

The process X = (Xn) represents the gap between the lengths of the at-

tacker’s chain and the (worst-case) main chain, in favor of the latter, as the nth

lengthening of the latter occurred.

We claim that Y = (Yn) is a super-martingale, namely that for any history,

E[Yn+1 j Yn; :::; Y0] (cid:20) Yn. Indeed, while the value of Xn+1 depends naturally on

(cid:28)n+1; :::; (cid:28)1, the increment Xn+1 (cid:0) Xn given a history (cid:28)n; :::; (cid:28)1 is controlled by

the random variable (cid:28)n given this history, with the pdf f(cid:28)nj(cid:28)n(cid:0)1;:::;(cid:28)1 which we

abbreviate f . We have:

[(

(cid:13)

(cid:12)1

)Xn+1(cid:12)(cid:12)(cid:12)(

(

(cid:0)(cid:13)(cid:28)n+1((cid:13)(cid:28)n+1)k

e

k!

]

)X0

(

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

)Xn

)Xn+1(cid:0)k

(

; :::;

(cid:0)(cid:13)(cid:28)n+1 ((cid:13)(cid:28)n+1)k

e

f ((cid:28)n+1)

d(cid:28)n+1 =

=

(6)

(7)

(cid:13)

(cid:12)1

(

k!

1∑

k=0

d(cid:28)n+1 =

)(cid:0)k

(cid:13)

(cid:12)1

d(cid:28)n+1 =

)k

(cid:0)(cid:13)(cid:28)n+1

f ((cid:28)n+1)e

(cid:13)(cid:28)n+1

(cid:13)

(cid:12)1

k!

f ((cid:28)n+1)e

(cid:0)(cid:13)(cid:28)n+1e

(

(cid:13)

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:0)1

(cid:13)(cid:28)n+1

)

d(cid:28)n+1 =

(cid:28)n+1

d(cid:28)n+1 (cid:20)

(8)

f ((cid:28)n+1)e

k=0

k=0

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

f ((cid:28)n+1)

∫ 1

E [Yn+1jYn; :::; Y0] = E

1∑

)Xn+1 (cid:1)

(

∫ 1

1∑

(

)Xn+1 (cid:1)

∫ 1

)Xn+1 (cid:1)

(

∫ 1

(

)Xn (cid:1) (cid:13)

∫ 1

)Xn

(

(

)Xn

= Yn:

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:12)1

(cid:1)

Equality 6 is due to the attacker’s chain advancing during the waiting time

(cid:28)n+1 according to a Poisson process with rate (cid:28)n+1 (cid:1) (cid:13). In 7 we made explicit

(cid:13)

(cid:12)1

the fact that

is a constant in the (cid:27)-algebra corresponding to the natural

(cid:12)ltration (usually denoted by (cid:27)(Xn; :::; X1)). Finally, as a corollary of Lemma 14,

j (cid:28)n(cid:0)1; :::; (cid:28)1] (cid:20) k!(cid:12)

(cid:0)k

E[(cid:28) k

1 . Combining this with the end of Lemma 15’s

n

proof shows that H(cid:13);(cid:12)1( (cid:13)

(cid:12)1

Let x1 < X0 < x2 be some (cid:12)xed constants, let the stopping time (cid:25) be de(cid:12)ned

by (cid:25) := minfn j Xn (cid:20) x1 _ Xn (cid:21) x2g, and (cid:12)nally, de(cid:12)ne the event Ex1;x2 :=

fX(cid:25) = x2g (i.e., \X reached x2 before it reached x1"). By Doob’s Optional

(cid:0)k (cid:20) k!(cid:12)

) (cid:20) 0, hence 8.

Stopping Theorem (See [19], p. 100-101) applied to the super martingale Y , we

have,

(

)X0

)X0 (cid:0)

(

= Y0 (cid:21) E[Y(cid:25)] =

(cid:13)

(cid:12)1

(

(

P r(Ex1;x2) (cid:1)

(cid:13)

(cid:12)1

) (cid:1)

x1;x2

+ P r(Ec

)x2

)x1 (cid:21) P r(Ex1;x2) (cid:1)

)X0 (cid:0)

)x1

)x1 :

)x2 (cid:0)

(

(

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(cid:13)

(cid:12)1

(

(

P r(Ex1;x2) (cid:21)

(cid:13)

(cid:12)1

(

((

(cid:13)

(cid:12)1

)x1

)x2 (cid:0)

(

=)

(cid:13)

(cid:12)1

)x1

)

(cid:13)

(cid:12)1

=)

Taking x1 = (cid:0)1 and x2 ! 1 we obtain a lower bound on the probability

)X0+1. The

⊓⊔

that the gap between the chains will never reach minus 1: 1 (cid:0) ( (cid:13)

(cid:12)1

success probability of an attack is thus upper bounded by ( (cid:13)

)X0+1.

(cid:12)1

are i.i.d then there exists an a0 (cid:20) (cid:13)

(cid:12)1

Note that an almost identical method shows that if the random variables (cid:28)n

such that Y := aX

0 is a martingale.

